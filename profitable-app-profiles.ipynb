{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profitable App Profiles\n",
    "### Hemanth Soni, June 2020\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction and Overview\n",
    "\n",
    "The goal of this project is to identify the most profitable app profiles in the store. This should help our agency identify where we should focus our development effort. In order to ensure only relevant data is analyzed, the characteristics of the agency need to be kept in mind:\n",
    "* Only builds free apps (no paid apps)\n",
    "* Only builds apps for the English-speaking world (no foreign-language apps)\n",
    "\n",
    "Typically, I wouldn't want to exclude data outside of this profile (as I may find that those excluded categories / formats are actually the most lucrative) but for the purposes of this exercise I'll take those constraints for granted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets\n",
    "\n",
    "First, I'm going to start by importing a few datasets. The tutorial I am following provides two:\n",
    "* [9660 Android apps](https://www.kaggle.com/lava18/google-play-store-apps)\n",
    "* [7195 iOS apps](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps)\n",
    "\n",
    "Separately, I was able to find a [third much larger dataset of Android apps on Kaggle](https://www.kaggle.com/gauthamp10/google-playstore-apps?select=Google-Playstore-Full.csv). It has the same fields available in the provided Android dataset, so I'm going to also include this in the analysis. The larger dataset should allow for more granular insights into the Android market. Unfortunately, a similar larger dataset couldn't be found for the Apple app store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "#The small Google dataset\n",
    "open_file = open('apps_datasets/google_small.csv', encoding='utf8')\n",
    "read_file = reader(open_file)\n",
    "googlesmall = list(read_file)\n",
    "googlesmall_header = googlesmall[0]\n",
    "googlesmall_table = googlesmall[1:]\n",
    "\n",
    "#The large Google dataset\n",
    "open_file = open('apps_datasets/google_large.csv', encoding='utf8')\n",
    "read_file = reader(open_file)\n",
    "googlelarge = list(read_file)\n",
    "googlelarge_header = googlelarge[0]\n",
    "googlelarge_table = googlelarge[1:]\n",
    "\n",
    "#The Apple dataset\n",
    "open_file = open('apps_datasets/apple.csv', encoding='utf8')\n",
    "read_file = reader(open_file)\n",
    "apple = list(read_file)\n",
    "apple_header = apple[0]\n",
    "apple_table = apple[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this data easier to explore, I first wrote a function that makes it easier to 'peek' into a dataset in a readable way. This function lets me print any number of rows from each of the datasets and get a view into the datasets total number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data (dataset, start, end, overview=True, hasHeader=True):\n",
    "    slice = dataset[start:end]\n",
    "    \n",
    "    print('Overview of first ' + str(end-start) + ' rows in database')\n",
    "    print('\\n')\n",
    "    \n",
    "    for each in slice:\n",
    "        print(each)\n",
    "        print('\\n')\n",
    "        \n",
    "    if overview == True:\n",
    "        if hasHeader == True:\n",
    "            print('Number of columns = ' + str(len(dataset[0])))\n",
    "            print('Number of rows = ' + str(len(dataset)-1))\n",
    "            print('-'*40)\n",
    "        else:\n",
    "            print('Number of columns = ' + str(len(dataset[0])))\n",
    "            print('Number of rows = ' + str(len(dataset)))\n",
    "            print('-'*40)\n",
    "            \n",
    "explore_data(googlesmall,0,5)\n",
    "explore_data(googlelarge,0,5)\n",
    "explore_data(apple,0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "### Manually correcting known error\n",
    "\n",
    "Based on a [discussion](https://www.kaggle.com/lava18/google-play-store-apps/discussion/66015) of one of the datasets, there appears to be a known error in the small Google Play Store dataset. We can correct for this by filling in the data by finding [the app](https://play.google.com/store/apps/details?id=com.lifemade.internetPhotoframe) in the Play Store and filling in the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the app based on the comments section and printing it to ensure it matches the expected error row.\n",
    "print(googlesmall[10473])\n",
    "\n",
    "# Printing another row that is known to be fine to understand where the issue lays.\n",
    "print(googlesmall[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing these two outputs, we can see that the \"category\" (index position 1) is missing in the error row. We can correct for this by adding it into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlesmall[10473].insert(1,'LIFESTYLE')\n",
    "\n",
    "print(googlesmall[10473])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates\n",
    "\n",
    "Generally, it's a good idea to check for duplicates in the datasets, and remove them if they exist. We will do this as a two step process.\n",
    "1. Check if the database has duplicates\n",
    "2. Remove the duplicates\n",
    "\n",
    "We could theoretically skip step 1, but we'll do it anyways since this is meant to be a learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique apps:  7198\n",
      "Number of duplicate apps:  0\n",
      "Number of unique apps:  9661\n",
      "Number of duplicate apps:  1181\n"
     ]
    }
   ],
   "source": [
    "duplicate_apps = []\n",
    "unique_apps = []\n",
    "\n",
    "def check_dupes(listname):\n",
    "    for each in listname:\n",
    "        name = each[0]\n",
    "        if name in unique_apps:\n",
    "            duplicate_apps.append(name)\n",
    "        else:\n",
    "            unique_apps.append(name)\n",
    "            \n",
    "    print('Number of unique apps: ',len(unique_apps))\n",
    "    print('Number of duplicate apps: ',len(duplicate_apps))\n",
    "    \n",
    "    del unique_apps[:]\n",
    "    del duplicate_apps[:]\n",
    "    \n",
    "# Checking each list for duplicates\n",
    "\n",
    "check_dupes(apple)\n",
    "check_dupes(googlesmall)\n",
    "\n",
    "#The check for the large database is disabled as my computer isn't strong enough to run it.\n",
    "#check_duples(googlelarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that the Apple Store dataset doesn't have any duplicates for us to worry about, but the smaller Google Play Store dataset does. We'll filter through this list and keep only the version of each app with the most reviews (as this suggests the most complete and up-to-date data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
